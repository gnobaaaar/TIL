# Nomad Web Scrapper
#TIL/python

1. ëª¨ë“ˆì„¤ì¹˜
2. ê°€ì ¸ì˜¬ í˜ì´ì§€ì˜ urlìš”ì²­ (get .text)
3. ì›í•˜ëŠ” html íŒŒíŠ¸ ê°€ì ¸ì˜¤ê¸° (Beautiful Soup)
4. pagination ì°¾ê¸°
5. pagination ì•ˆì˜ ëª¨ë“  ì•µì»¤ ì°¾ê¸°
loopë¥¼ ì´ìš©í•´ ê° í˜ì´ì§€ì˜ â€œspanâ€ ëª¨ë‘ ì°¾ê¸°
6. ë¶ˆëŸ¬ì˜¬ í˜ì´ì§€ ë„˜ë²„ ì§€ì •í•´ì£¼ê¸°

#1. Import Packages (ëª¨ë“ˆì„¤ì¹˜)
	* ì˜ìƒì—ì„œ ì“°ì¸ ëª¨ë“ˆ :
	ã„´Request (ì‚¬ì´íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (text))
	ã„´Beautiful Soup (html ë‚´ í•„ìš”í•œ ë¶€ë¶„ ì¶”ì¶œí•˜ê¸° (html))
#2. ê°€ì ¸ì˜¬ í˜ì´ì§€ì˜ urlìš”ì²­ (request.get)
	* í˜ì´ì§€.text ê°€ì ¸ì™•
#3. ì›í•˜ëŠ” html íŒŒíŠ¸ ê°€ì ¸ì˜¤ê¸° (Beautiful Soup)
	* ìœ„ .textì—ì„œ HTML ë¶ˆëŸ¬ì™• (html.parser)
#4. HTML ë‚´ì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì •ë³´ì˜ pagination(í˜ì´ì§€ ë„¤ë¹„ê²Œì´í„°)ì„ ì°¾ê¸°(indeed_soup.find)
	ã„´â€divâ€ ë¥¼ ì°¾ì•„ì„œ â€œpaginationâ€ í´ë˜ìŠ¤ ë¶ˆëŸ¬ì™€ì¤­
#5-1. pagination ì•ˆì˜ ëª¨ë“  ì•µì»¤(â€˜a hrefâ€™ í˜•íƒœë¡œ ë˜ì–´ìˆëŠ” ë§í¬ë“¤) ì°¾ì•„ì£¼ê¸°
	ã„´pagination.find_all(â€˜aâ€™))
#5-2. í˜ì´ì§€ ë§í¬ë§ˆë‹¤ ìˆëŠ” íƒœê·¸ë¥¼ ê°ê° ëª¨ë‘ ë¶ˆëŸ¬ì™€ì¤˜ì•¼ í•˜ë¯€ë¡œ loop(for-in) ì‚¬ìš©
	ã„´for link in links: pages.append(link.find(â€œspanâ€))
#6. ì–´ë””ì„œë¶€í„° ì–´ë””ê¹Œì§€ ë¶ˆëŸ¬ì˜¬ê±´ì§€ í˜ì´ì§€ ë„˜ë²„ ì§€ì •í•´ì£¼ê¸°
	ã„´pages = pages[0:-1]

<br />

<br />

## 

## BeautifulSoup
[Beautiful Soup Documentation â€” Beautiful Soup 4.9.0 documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)

![E752C819-5E31-4B38-8A83-2A1096098FFA](image/E752C819-5E31-4B38-8A83-2A1096098FFA.png)

```python
import requests
from bs4 import BeautifulSoup 

indeed_result = requests.get(
  "https://www.indeed.com/jobs?q=python&limit=50")

indeed_soup = BeautifulSoup(indeed_result.text, "html.parser")

pagination = indeed_soup.find("ul", {"class":"pagination-list"})

pages = pagination.find_all('a')
spans = []

for page in pages:
  spans.append(page.find("span"))

spans = spans[:-1]
```

ê²°ê³¼í™”ë©´

![D2C97610-1A0C-448E-A8CD-5062F697C240](image/D2C97610-1A0C-448E-A8CD-5062F697C240.png)

<br />

<br />

## 

## í•¨ìˆ˜ë¡œ ë¶„ë¥˜í•˜ê¸°

![ED5790EC-756D-4F2D-A656-4D90DBB9A6C8](image/ED5790EC-756D-4F2D-A656-4D90DBB9A6C8.png)

indeed.py

```python
import requests
from bs4 import BeautifulSoup 

LIMIT = 50
URL = f"https://www.indeed.com/jobs?q=python&limit={LIMIT}"

def extract_indeed_pages():
  result = requests.get(URL)

  soup = BeautifulSoup(result.text, "html.parser")

  pagination = soup.find("ul", {"class":"pagination-list"})

  links = pagination.find_all('a')
  pages = []

  for link in links[:-1]:
    pages.append(int(link.string))

  pages = pages[:-1]
  max_page = pages[-1]
  # 5ëŠ” ì¸ì‹í•˜ì§€ ëª»í•œë‹¤ -> why?
  return max_page


def extract_indeed_jobs(last_pages):
  for page in range(last_pages):
    result = requests.get(f"{URL}&start={page*LIMIT}")
    
    print(result.status_code)
```


main.py
```python
from indeed import extract_indeed_pages, extract_indeed_jobs

last_indeed_page = extract_indeed_pages()
extract_indeed_jobs(last_indeed_page)
```

<br />

<br />

## day5 ì±Œë¦°ì§€

êµ­ê°€ì½”ë“œ ìŠ¤í¬ë˜í•‘

![Aug-14-2021 02-52-11](image/Aug-14-2021 02-52-11.gif)

```python
import os
import requests
from bs4 import BeautifulSoup

os.system("clear")
URL = "https://www.iban.com/currency-codes"
  
def country_list():
  result = requests.get(URL)
  soup = BeautifulSoup(result.text, "html.parser")

  links = soup.find_all('tr')
  list=[]

  for i in links[1:]:
    tmp = i.find_all('td')
    tmp_list = []
    if tmp[3]:
      tmp_list.append((tmp[0].string).capitalize())
      tmp_list.append(tmp[2].string)
    list.append(tmp_list)
  
  return list

def user_input(list):
  print("Hello! Please choose select a country by number:")
  for fin_nation in enumerate(list):
    print(f"# {fin_nation[0]} {fin_nation[1][0]}")

  # ì‚¬ìš©ì ì…ë ¥í¼
  while(1):
    try:
      number = int(input())
      print(f"You chose {list[number][0]}.")
      print(f"The currency code is {list[number][1]}.")
      print("DONE!ğŸ˜„")
      break
    except IndexError:
      print("Choose a number from the list.")
      continue
    except ValueError:
      print("That wasn't a number.")
      continue
```





## ì±Œë¦°ì§€ day

ì•Œë°”ì²œêµ­ ìŠˆí¼ë¸Œëœë“œ ì‚¬ì´íŠ¸ ì›¹ ìŠ¤í¬ë˜í•‘í•˜ê¸°

![Aug-17-2021 03-20-46](image/Aug-17-2021 03-20-46.gif)

![image-20210817032152870](image/image-20210817032152870.png)

```python
import os
import csv
import requests
from bs4 import BeautifulSoup

os.system("clear")
alba_url = "http://www.alba.co.kr"

# ìŠˆí¼ë¸Œëœë“œ urls
def company_list():
  result = requests.get(alba_url)
  soup = BeautifulSoup(result.text, "html.parser")
  list = soup.find('div', {'id':'MainSuperBrand'})
  list2 = list.find("ul", {"class": "goodsBox"})
  list3 = list2.find_all("li")[:-3]

  company_name = []
  company_url =[]

  for i in list3:
    company_name.append(i.find("span", {"class":"company"}).text)
    company_url.append(i.find("a", {"class":"goodsBox-info"})["href"])

  result = []

  for i in range(len(company_name)):
    mini = []
    mini.append(company_name[i])
    mini.append(company_url[i])
    result.append(mini)

  return result


# ê° íšŒì‚¬ í•­ëª© detail
def collect_company(url_list):
  com_url = url_list[1]
  result = requests.get(com_url)
  soup = BeautifulSoup(result.text, "html.parser")
  list = soup.find('tbody')

  place_list = list.find_all('td', {'class':'local'})
  title_list = list.find_all('span', {'class':'company'})
  time_list = list.find_all('td', {'class':'data'})
  pay_list = list.find_all('td', {'class':'pay'})
  date_list = list.find_all('td', {'class':'regDate'})

  # ì§€ì—­
  place = []
  for i in place_list:
    i = i.text
    i = i.replace('\xa0', ' ')
    place.append(i)
  
  # ë§¤ì¥ëª…
  title = []
  for i in title_list:
    i = i.text
    title.append(i)

  # time
  time = []
  for i in time_list:
    i = i.text
    time.append(i)
  
  # pay
  pay = []
  for i in pay_list:
    i = i.text
    pay.append(i)

  # date
  date = []
  for i in date_list:
    i = i.text
    date.append(i)

  result = []
  for i in range(len(place)):
    mini_result = []
    mini_result.append(place[i])
    mini_result.append(title[i])
    mini_result.append(time[i])
    mini_result.append(pay[i])
    mini_result.append(date[i])
    result.append(mini_result)

  return result


def save_to_file(lists, name):
  file_name = name[0]
  file_name = file_name.replace('/', '_') 
  file = open(f"{file_name}.csv",mode="w")
  writer = csv.writer(file)
  writer.writerow(["place", "title", "time", "pay", "date"])
  for company in lists:
    writer.writerow(list(company))
  return



url_list = company_list()

for url in url_list:
  company = collect_company(url)
  save_to_file(company, url)
```

