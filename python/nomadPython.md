# Nomad Web Scrapper
#TIL/python

1. ëª¨ë“ˆì„¤ì¹˜
2. ê°€ì ¸ì˜¬ í˜ì´ì§€ì˜ urlìš”ì²­ (get .text)
3. ì›í•˜ëŠ” html íŒŒíŠ¸ ê°€ì ¸ì˜¤ê¸° (Beautiful Soup)
4. pagination ì°¾ê¸°
5. pagination ì•ˆì˜ ëª¨ë“  ì•µì»¤ ì°¾ê¸°
loopë¥¼ ì´ìš©í•´ ê° í˜ì´ì§€ì˜ â€œspanâ€ ëª¨ë‘ ì°¾ê¸°
6. ë¶ˆëŸ¬ì˜¬ í˜ì´ì§€ ë„˜ë²„ ì§€ì •í•´ì£¼ê¸°

#1. Import Packages (ëª¨ë“ˆì„¤ì¹˜)
	* ì˜ìƒì—ì„œ ì“°ì¸ ëª¨ë“ˆ :
	ã„´Request (ì‚¬ì´íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (text))
	ã„´Beautiful Soup (html ë‚´ í•„ìš”í•œ ë¶€ë¶„ ì¶”ì¶œí•˜ê¸° (html))
#2. ê°€ì ¸ì˜¬ í˜ì´ì§€ì˜ urlìš”ì²­ (request.get)
	* í˜ì´ì§€.text ê°€ì ¸ì™•
#3. ì›í•˜ëŠ” html íŒŒíŠ¸ ê°€ì ¸ì˜¤ê¸° (Beautiful Soup)
	* ìœ„ .textì—ì„œ HTML ë¶ˆëŸ¬ì™• (html.parser)
#4. HTML ë‚´ì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì •ë³´ì˜ pagination(í˜ì´ì§€ ë„¤ë¹„ê²Œì´í„°)ì„ ì°¾ê¸°(indeed_soup.find)
	ã„´â€divâ€ ë¥¼ ì°¾ì•„ì„œ â€œpaginationâ€ í´ë˜ìŠ¤ ë¶ˆëŸ¬ì™€ì¤­
#5-1. pagination ì•ˆì˜ ëª¨ë“  ì•µì»¤(â€˜a hrefâ€™ í˜•íƒœë¡œ ë˜ì–´ìˆëŠ” ë§í¬ë“¤) ì°¾ì•„ì£¼ê¸°
	ã„´pagination.find_all(â€˜aâ€™))
#5-2. í˜ì´ì§€ ë§í¬ë§ˆë‹¤ ìˆëŠ” íƒœê·¸ë¥¼ ê°ê° ëª¨ë‘ ë¶ˆëŸ¬ì™€ì¤˜ì•¼ í•˜ë¯€ë¡œ loop(for-in) ì‚¬ìš©
	ã„´for link in links: pages.append(link.find(â€œspanâ€))
#6. ì–´ë””ì„œë¶€í„° ì–´ë””ê¹Œì§€ ë¶ˆëŸ¬ì˜¬ê±´ì§€ í˜ì´ì§€ ë„˜ë²„ ì§€ì •í•´ì£¼ê¸°
	ã„´pages = pages[0:-1]

<br />

<br />

## 

## BeautifulSoup
[Beautiful Soup Documentation â€” Beautiful Soup 4.9.0 documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)

![E752C819-5E31-4B38-8A83-2A1096098FFA](image/E752C819-5E31-4B38-8A83-2A1096098FFA.png)

```python
import requests
from bs4 import BeautifulSoup 

indeed_result = requests.get(
  "https://www.indeed.com/jobs?q=python&limit=50")

indeed_soup = BeautifulSoup(indeed_result.text, "html.parser")

pagination = indeed_soup.find("ul", {"class":"pagination-list"})

pages = pagination.find_all('a')
spans = []

for page in pages:
  spans.append(page.find("span"))

spans = spans[:-1]
```

ê²°ê³¼í™”ë©´

![D2C97610-1A0C-448E-A8CD-5062F697C240](image/D2C97610-1A0C-448E-A8CD-5062F697C240.png)

<br />

<br />

## 

## í•¨ìˆ˜ë¡œ ë¶„ë¥˜í•˜ê¸°

![ED5790EC-756D-4F2D-A656-4D90DBB9A6C8](image/ED5790EC-756D-4F2D-A656-4D90DBB9A6C8.png)

indeed.py

```python
import requests
from bs4 import BeautifulSoup 

LIMIT = 50
URL = f"https://www.indeed.com/jobs?q=python&limit={LIMIT}"

def extract_indeed_pages():
  result = requests.get(URL)

  soup = BeautifulSoup(result.text, "html.parser")

  pagination = soup.find("ul", {"class":"pagination-list"})

  links = pagination.find_all('a')
  pages = []

  for link in links[:-1]:
    pages.append(int(link.string))

  pages = pages[:-1]
  max_page = pages[-1]
  # 5ëŠ” ì¸ì‹í•˜ì§€ ëª»í•œë‹¤ -> why?
  return max_page


def extract_indeed_jobs(last_pages):
  for page in range(last_pages):
    result = requests.get(f"{URL}&start={page*LIMIT}")
    
    print(result.status_code)
```


main.py
```python
from indeed import extract_indeed_pages, extract_indeed_jobs

last_indeed_page = extract_indeed_pages()
extract_indeed_jobs(last_indeed_page)
```

<br />

<br />

## day5 ì±Œë¦°ì§€

êµ­ê°€ì½”ë“œ ìŠ¤í¬ë˜í•‘

![Aug-14-2021 02-52-11](image/Aug-14-2021 02-52-11.gif)

```python
import os
import requests
from bs4 import BeautifulSoup

os.system("clear")
URL = "https://www.iban.com/currency-codes"
  
def country_list():
  result = requests.get(URL)
  soup = BeautifulSoup(result.text, "html.parser")

  links = soup.find_all('tr')
  list=[]

  for i in links[1:]:
    tmp = i.find_all('td')
    tmp_list = []
    if tmp[3]:
      tmp_list.append((tmp[0].string).capitalize())
      tmp_list.append(tmp[2].string)
    list.append(tmp_list)
  
  return list

def user_input(list):
  print("Hello! Please choose select a country by number:")
  for fin_nation in enumerate(list):
    print(f"# {fin_nation[0]} {fin_nation[1][0]}")

  # ì‚¬ìš©ì ì…ë ¥í¼
  while(1):
    try:
      number = int(input())
      print(f"You chose {list[number][0]}.")
      print(f"The currency code is {list[number][1]}.")
      print("DONE!ğŸ˜„")
      break
    except IndexError:
      print("Choose a number from the list.")
      continue
    except ValueError:
      print("That wasn't a number.")
      continue
```